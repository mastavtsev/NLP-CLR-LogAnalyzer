{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879454d8-632e-4e23-b9cd-707046115393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install torch~=2.2.2 transformers~=4.39.3 pm4py~=2.7.11.4 pandas~=2.2.1 matplotlib~=3.8.3 numpy~=1.25.2 tqdm~=4.66.2 Pillow~=10.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14c10d73-bf9f-4cb5-9017-2ef4ad206737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:21.796653Z",
     "iopub.status.busy": "2024-06-26T17:13:21.795205Z",
     "iopub.status.idle": "2024-06-26T17:13:21.806241Z",
     "shell.execute_reply": "2024-06-26T17:13:21.805522Z",
     "shell.execute_reply.started": "2024-06-26T17:13:21.796617Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "__file__ = '/home/jupyter/datasphere/project/application.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0cb580aa-c121-40f9-bf35-0b32d85e901d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:22.331865Z",
     "iopub.status.busy": "2024-06-26T17:13:22.330661Z",
     "iopub.status.idle": "2024-06-26T17:13:22.411084Z",
     "shell.execute_reply": "2024-06-26T17:13:22.410310Z",
     "shell.execute_reply.started": "2024-06-26T17:13:22.331827Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class FileManager:\n",
    "    @staticmethod\n",
    "    def get_filename(xes_file):\n",
    "        base_name = os.path.basename(xes_file)\n",
    "        file_name, _ = os.path.splitext(base_name)\n",
    "        return file_name\n",
    "\n",
    "    @staticmethod\n",
    "    def get_save_path(default_filename, default_extension):\n",
    "        '''\n",
    "        :param default_filename: file name with extension\n",
    "        :param default_extension: str extension with dot in front, like \".png\"\n",
    "        :return: path to file\n",
    "        '''\n",
    "        save_path = input('Provide a file path with extension to save the file:')\n",
    "        return save_path\n",
    "\n",
    "    @staticmethod\n",
    "    def get_in_path(base_dir, default_extension):\n",
    "        '''\n",
    "        :param base_dir: directory to start searching files\n",
    "        :param default_extension: str extension with dot in front, like \".txt\"\n",
    "        :return: path to file\n",
    "        '''\n",
    "        in_path = input('Provide a file path: ')\n",
    "        return in_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c4ad958-036b-45b3-afab-b82b8727a1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:22.696013Z",
     "iopub.status.busy": "2024-06-26T17:13:22.695073Z",
     "iopub.status.idle": "2024-06-26T17:13:22.720975Z",
     "shell.execute_reply": "2024-06-26T17:13:22.720202Z",
     "shell.execute_reply.started": "2024-06-26T17:13:22.695978Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pm4py\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class XESTracesProcessor:\n",
    "    def __init__(self, input_filepath, output_filepath=None, num_needed_indexes=25):\n",
    "        self.input_filepath = input_filepath\n",
    "        self.output_filepath = output_filepath\n",
    "        self.num_needed_indexes = num_needed_indexes\n",
    "\n",
    "    def process_file(self):\n",
    "        print('\\033[93m' + \"processing log...\" + '\\033[0m')\n",
    "        \n",
    "        log = self.__get_log(self.input_filepath)\n",
    "        \n",
    "        event_log = self.__get_needed_columns_log(log)\n",
    "        \n",
    "        needed_indexes = self.__get_needed_indexes(event_log[event_log[\"ManagedThreadId\"] != -1])\n",
    "\n",
    "        outliers = self.__get_outliers(event_log, needed_indexes)\n",
    "\n",
    "        event_log = event_log[event_log['ManagedThreadId'].isin(needed_indexes)]\n",
    "        traces_log = self.__get_traces_log(event_log)\n",
    "        final_trace_log = self.__get_final_log(traces_log, outliers)\n",
    "\n",
    "        if self.output_filepath is not None:\n",
    "            self.__write_traces_to_file(final_trace_log, self.output_filepath)\n",
    "\n",
    "        return self.__get_list_of_traces(final_trace_log)\n",
    "\n",
    "    def __get_needed_indexes(self, event_log):\n",
    "        value_counts = event_log['ManagedThreadId'].value_counts()\n",
    "        needed_indexes = list(value_counts[:self.num_needed_indexes].index)\n",
    "\n",
    "        return needed_indexes\n",
    "\n",
    "    def __get_log(self, filepath):\n",
    "        return pm4py.read_xes(filepath)\n",
    "\n",
    "    def __get_needed_columns_log(self, log):\n",
    "        event_log = log[['ManagedThreadId', 'concept:name', 'time:timestamp']]\n",
    "        event_log = event_log.astype({\"ManagedThreadId\": int, 'concept:name': str, 'time:timestamp': str})\n",
    "\n",
    "        regex = r'(\\d{2}:\\d{2}:\\d{2}.\\d{6}\\+\\d{2}:\\d{2})'\n",
    "        event_log['time:timestamp'] = event_log['time:timestamp'].str.extract(regex)\n",
    "        event_log['time:timestamp'] = pd.to_datetime(event_log['time:timestamp'])\n",
    "        return event_log\n",
    "\n",
    "    def __get_outliers(self, event_log, needed_indexes):\n",
    "        outliers = event_log[~event_log['ManagedThreadId'].isin(needed_indexes)]\n",
    "        return outliers\n",
    "\n",
    "    def __get_traces_log(self, event_log):\n",
    "        event_log = event_log.copy()\n",
    "        event_log['time:timestamp'] = event_log['time:timestamp'].dt.tz_localize(None)\n",
    "        traces = event_log.groupby('ManagedThreadId').apply(\n",
    "            lambda x: [[row['concept:name'], row['time:timestamp']] for index, row in x.iterrows()]\n",
    "        )\n",
    "        start_times = event_log.groupby('ManagedThreadId')['time:timestamp'].min()\n",
    "        end_times = event_log.groupby('ManagedThreadId')['time:timestamp'].max()\n",
    "        traces_log = pd.DataFrame({\n",
    "            'ManagedThreadId': traces.index,\n",
    "            'Trace': traces.values,\n",
    "            'Start Time': start_times.values,\n",
    "            'End Time': end_times.values\n",
    "        })\n",
    "        return traces_log\n",
    "\n",
    "    def __get_final_log(self, traces_log, outliers):\n",
    "        traces_log = traces_log.copy()\n",
    "        intervals = pd.arrays.IntervalArray.from_arrays(traces_log['Start Time'], traces_log['End Time'], closed='both')\n",
    "        outliers = outliers.copy()\n",
    "        outliers['time:timestamp'] = outliers['time:timestamp'].dt.tz_localize(None)\n",
    "\n",
    "        for idx, row in tqdm(outliers.iterrows(), desc=\"creating dataframe\", total=len(outliers)):\n",
    "            timestamp = row['time:timestamp']\n",
    "            concept_name = row['concept:name']\n",
    "            pair = [concept_name, timestamp]\n",
    "            mask = intervals.contains(timestamp)\n",
    "            trace_indices = traces_log[mask].index\n",
    "            for trace_idx in trace_indices:\n",
    "                traces_log.at[trace_idx, 'Trace'].append(pair)\n",
    "\n",
    "        def sort_trace(trace):\n",
    "            filtered_trace = [event for event in trace if event[1] is not None]\n",
    "            return [event for event, timestamp in sorted(filtered_trace, key=lambda elem_pair: elem_pair[1])]\n",
    "\n",
    "        traces_log['Trace'] = traces_log['Trace'].apply(sort_trace)\n",
    "\n",
    "        return traces_log\n",
    "\n",
    "    def __write_traces_to_file(self, log, filepath):\n",
    "        with open(filepath, \"a\") as file:\n",
    "            for trace in log[\"Trace\"]:\n",
    "                file.write(' '.join(trace) + \"\\n\")\n",
    "\n",
    "    def __get_list_of_traces(self, log):\n",
    "        traces = []\n",
    "        for trace in log[\"Trace\"]:\n",
    "            traces.append(trace)\n",
    "        return traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dcbf5956-d31d-4f2f-a70c-87de74f48318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:22.985752Z",
     "iopub.status.busy": "2024-06-26T17:13:22.984882Z",
     "iopub.status.idle": "2024-06-26T17:13:23.003988Z",
     "shell.execute_reply": "2024-06-26T17:13:23.003248Z",
     "shell.execute_reply.started": "2024-06-26T17:13:22.985702Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "\n",
    "\n",
    "class LogProcessor:\n",
    "    max_LoA = 13\n",
    "\n",
    "    def __init__(self,\n",
    "                 xes_path,\n",
    "                 case_id='ManagedThreadId',\n",
    "                 activity_key='concept:name',\n",
    "                 timestamp_key='time:timestamp'\n",
    "                 ):\n",
    "\n",
    "        self.xes_path = xes_path\n",
    "\n",
    "        self.case_id = case_id\n",
    "        self.activity_key = activity_key\n",
    "        self.timestamp_key = timestamp_key\n",
    "\n",
    "        self.tokenized_log = None\n",
    "        self.LoA = None\n",
    "\n",
    "        self.traces_processor = TracesProcessor()\n",
    "        self.traces = None\n",
    "\n",
    "    def __extract_traces(self):\n",
    "        processor = XESTracesProcessor(self.xes_path)\n",
    "        self.traces = processor.process_file()\n",
    "\n",
    "    def __process_trace(self, case_id, tokens, start_time):\n",
    "        time = start_time\n",
    "        prev_act = None\n",
    "        events = []\n",
    "        for activity in tokens:\n",
    "            if activity != prev_act:\n",
    "                events.append({\n",
    "                    \"case:concept:name\": case_id,\n",
    "                    \"concept:name\": activity,\n",
    "                    \"time:timestamp\": time\n",
    "                })\n",
    "                time += timedelta(minutes=1)  # +1 minute to time of past event\n",
    "                prev_act = activity\n",
    "        return events\n",
    "\n",
    "    def create_tokenized_event_log(self, LoA):\n",
    "        if LoA <= 0 or LoA > self.max_LoA:\n",
    "            raise ValueError(f\"LOA must be between 1 and {self.max_LoA}\")\n",
    "\n",
    "        if self.tokenized_log is not None and self.LoA == LoA:\n",
    "            return self.tokenized_log\n",
    "\n",
    "        if not self.traces:\n",
    "            self.__extract_traces()\n",
    "\n",
    "        processed_traces = self.traces_processor.process_traces(self.traces, LoA)\n",
    "        events = []\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        print('\\033[93m' + \"finishing dataframe creation ...\" + '\\033[0m')\n",
    "        \n",
    "        for case_id, tokens in enumerate(processed_traces, start=1):\n",
    "            time = start_time\n",
    "            prev_act = None\n",
    "            for activity in tokens:\n",
    "                if activity != prev_act:\n",
    "                    events.append({\n",
    "                        \"case:concept:name\": case_id,\n",
    "                        \"concept:name\": activity,\n",
    "                        \"time:timestamp\": time\n",
    "                    })\n",
    "                    time += timedelta(minutes=1)\n",
    "                    prev_act = activity\n",
    "\n",
    "        log_df = pd.DataFrame(events)\n",
    "\n",
    "        format_df = pm4py.format_dataframe(log_df, case_id='case:concept:name',\n",
    "                                           activity_key='concept:name',\n",
    "                                           timestamp_key='time:timestamp')\n",
    "\n",
    "        self.tokenized_log = pm4py.convert_to_event_log(format_df)\n",
    "        self.LoA = LoA\n",
    "\n",
    "        return self.tokenized_log\n",
    "\n",
    "    def get_traces(self):\n",
    "        if not self.traces:\n",
    "            self.__extract_traces()\n",
    "\n",
    "        return self.traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d3583ea5-8d0e-45fd-9d64-94a90e0a0b8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:23.288673Z",
     "iopub.status.busy": "2024-06-26T17:13:23.287506Z",
     "iopub.status.idle": "2024-06-26T17:13:23.309451Z",
     "shell.execute_reply": "2024-06-26T17:13:23.308687Z",
     "shell.execute_reply.started": "2024-06-26T17:13:23.288628Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "class TokenizerManager:\n",
    "    base_dir = os.path.dirname(__file__)\n",
    "    _base_dir = os.path.join(base_dir, 'tokenizers')\n",
    "    _paths = {\n",
    "        1: \"fast_bpe_l_5_v_512\",\n",
    "        2: \"fast_bpe_l_10_v_1109\",\n",
    "        3: \"fast_bpe_l_20_v_1707\",\n",
    "        4: \"fast_bpe_l_25_v_2304\",\n",
    "        5: \"fast_bpe_l_30_v_2901\",\n",
    "        6: \"fast_bpe_l_40_v_3499\",\n",
    "        7: \"fast_bpe_l_60_v_4096\",\n",
    "        8: \"fast_bpe_l_NONE_v_5000\",\n",
    "        9: \"fast_bpe_l_NONE_v_12500\",\n",
    "        10: \"fast_bpe_l_NONE_v_20000\",\n",
    "        11: \"fast_bpe_l_100_v_5000\",\n",
    "        12: \"fast_bpe_l_200_v_12500\",\n",
    "        13: \"fast_bpe_l_300_v_20000\"\n",
    "    }\n",
    "\n",
    "    _tokenizers = {}\n",
    "\n",
    "    _mappings = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tokenizer(level):\n",
    "        if level not in TokenizerManager._tokenizers:\n",
    "            tokenizer_dir = TokenizerManager._paths.get(level)\n",
    "            tokenizer_dir = os.path.join(TokenizerManager._base_dir, tokenizer_dir)\n",
    "            if tokenizer_dir:\n",
    "                TokenizerManager._tokenizers[level] = PreTrainedTokenizerFast.from_pretrained(tokenizer_dir)\n",
    "            else:\n",
    "                raise ValueError(f\"No tokenizer defined for level {level}\")\n",
    "        return TokenizerManager._tokenizers[level]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mapping(level):\n",
    "        if level not in TokenizerManager._mappings:\n",
    "            tokenizer_dir = TokenizerManager._paths.get(level)\n",
    "            mapping_path = os.path.join(TokenizerManager._base_dir, tokenizer_dir)\n",
    "            mapping_path = os.path.join(mapping_path, \"new_vocab_mapping_uni_rep.json\")\n",
    "            if mapping_path:\n",
    "                TokenizerManager._mappings[level] = TokenizerManager.__read_dict_from_json_file(mapping_path)\n",
    "            else:\n",
    "                raise ValueError(f\"No mapping defined for level {level}\")\n",
    "        return TokenizerManager._mappings[level]\n",
    "\n",
    "    @staticmethod\n",
    "    def __read_dict_from_json_file(filepath):\n",
    "        with open(filepath, 'r') as json_file:\n",
    "            dictionary = json.load(json_file)\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b8a99f4-c1e0-46b1-92b6-c05020e7cd79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:23.795028Z",
     "iopub.status.busy": "2024-06-26T17:13:23.793959Z",
     "iopub.status.idle": "2024-06-26T17:13:23.817293Z",
     "shell.execute_reply": "2024-06-26T17:13:23.816506Z",
     "shell.execute_reply.started": "2024-06-26T17:13:23.794978Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "class TraceDatabase:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        self._create_tables()\n",
    "\n",
    "    def _create_tables(self):\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS traces (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                trace_hash TEXT UNIQUE,\n",
    "                trace TEXT\n",
    "            )\n",
    "            ''')\n",
    "            cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS probs_scores (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                score REAL,\n",
    "                token TEXT,\n",
    "                trace_id INTEGER,\n",
    "                FOREIGN KEY (trace_id) REFERENCES traces (id)\n",
    "            )\n",
    "            ''')\n",
    "            cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS error_scores (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                score REAL,\n",
    "                token TEXT,\n",
    "                trace_id INTEGER,\n",
    "                FOREIGN KEY (trace_id) REFERENCES traces (id)\n",
    "            )\n",
    "            ''')\n",
    "            cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS brier_scores (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                score REAL,\n",
    "                token TEXT,\n",
    "                trace_id INTEGER,\n",
    "                FOREIGN KEY (trace_id) REFERENCES traces (id)\n",
    "            )\n",
    "            ''')\n",
    "            conn.commit()\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "\n",
    "    def hash_trace(self, trace):\n",
    "        trace_str = json.dumps(trace)\n",
    "        return hashlib.sha256(trace_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def get_trace(self, trace):\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            trace_hash = self.hash_trace(trace)\n",
    "            cursor.execute(\"SELECT id FROM traces WHERE trace_hash = ?\", (trace_hash,))\n",
    "            result = cursor.fetchone()\n",
    "            if result:\n",
    "                trace_id = result[0]\n",
    "                probs_anomalies = self._get_scores(cursor, \"probs_scores\", trace_id)\n",
    "                error_anomalies = self._get_scores(cursor, \"error_scores\", trace_id)\n",
    "                brier_scores = self._get_scores(cursor, \"brier_scores\", trace_id)\n",
    "                return trace_id, (probs_anomalies, error_anomalies, brier_scores)\n",
    "            else:\n",
    "                return None, None\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "\n",
    "    def _get_scores(self, cursor, table, trace_id):\n",
    "        cursor.execute(f\"SELECT score, token FROM {table} WHERE trace_id = ?\", (trace_id,))\n",
    "        rows = cursor.fetchall()\n",
    "        if table == \"brier_scores\":\n",
    "            scores_dict = {}\n",
    "            for score, token in rows:\n",
    "                if score not in scores_dict:\n",
    "                    scores_dict[score] = []\n",
    "                scores_dict[score].append(token)\n",
    "            return [(score, tokens) for score, tokens in scores_dict.items()]\n",
    "        else:\n",
    "            return [(row[0], row[1]) for row in rows]\n",
    "\n",
    "    def save_trace(self, trace, probs_anomalies, error_anomalies, brier_scores):\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            trace_hash = self.hash_trace(trace)\n",
    "            trace_str = json.dumps(trace)\n",
    "            cursor.execute(\"INSERT OR IGNORE INTO traces (trace_hash, trace) VALUES (?, ?)\", (trace_hash, trace_str))\n",
    "            trace_id = cursor.execute(\"SELECT id FROM traces WHERE trace_hash = ?\", (trace_hash,)).fetchone()[0]\n",
    "            self._save_scores(cursor, \"probs_scores\", trace_id, probs_anomalies)\n",
    "            self._save_scores(cursor, \"error_scores\", trace_id, error_anomalies)\n",
    "            self._save_scores(cursor, \"brier_scores\", trace_id, brier_scores)\n",
    "            conn.commit()\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "\n",
    "    def _save_scores(self, cursor, table, trace_id, scores):\n",
    "        for score, sample in scores:\n",
    "            if isinstance(sample, list):\n",
    "                for token in sample:\n",
    "                    cursor.execute(f\"INSERT INTO {table} (score, token, trace_id) VALUES (?, ?, ?)\", (score, token, trace_id))\n",
    "            else:\n",
    "                cursor.execute(f\"INSERT INTO {table} (score, token, trace_id) VALUES (?, ?, ?)\", (score, sample, trace_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b50d565e-0fcc-4cbc-b2c0-81ac931de6ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:23.819738Z",
     "iopub.status.busy": "2024-06-26T17:13:23.818652Z",
     "iopub.status.idle": "2024-06-26T17:13:23.853593Z",
     "shell.execute_reply": "2024-06-26T17:13:23.852794Z",
     "shell.execute_reply.started": "2024-06-26T17:13:23.819709Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BatchEncoding\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "class TraceEvaluatorDB:\n",
    "    def __init__(self, model, tokenizer, AET=0.05, APT=0.85, BS=0.5, mask_share=0.2, db_path=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token_id = self.tokenizer.mask_token_id\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.abnormal_error_threshold = AET\n",
    "        self.abnormal_prob_threshold = APT\n",
    "        self.brier_score_threshold = BS\n",
    "        self.mask_share = mask_share\n",
    "        self.db = TraceDatabase(db_path) if db_path else None\n",
    "\n",
    "    def evaluate_traces(self, traces: list[list[str]]):\n",
    "        if not all(isinstance(trace, list) and all(isinstance(elem, str) for elem in trace) for trace in traces):\n",
    "            raise ValueError(\"traces must be list of str lists\")\n",
    "\n",
    "        str_abnormal = \"abnormal\"\n",
    "        eval_results = []\n",
    "\n",
    "        for trace in tqdm(traces, desc='analyzing anomalies by model'):\n",
    "            trace_results = [[\"normal\", []] for _ in range(3)]\n",
    "\n",
    "            if self.db:\n",
    "                trace_id, db_results = self.db.get_trace(trace)\n",
    "            else:\n",
    "                trace_id, db_results = None, None\n",
    "\n",
    "            if db_results:\n",
    "                probs_anomalies, error_anomalies, brier_scores = db_results\n",
    "            else:\n",
    "                tokenized_trace_init = self.__preprocess_trace(trace)\n",
    "                tokens = self.tokenizer.convert_ids_to_tokens(tokenized_trace_init['input_ids'][0])\n",
    "                brier_scores = self.evaluate_trace_brier(tokenized_trace_init, tokens)\n",
    "                probs_anomalies, error_anomalies = self.evaluate_trace_by_tokens(tokenized_trace_init, tokens)\n",
    "\n",
    "                if self.db:\n",
    "                    self.db.save_trace(trace, probs_anomalies, error_anomalies, brier_scores)\n",
    "\n",
    "            if len(probs_anomalies) > 0:\n",
    "                trace_results[0] = [str_abnormal, probs_anomalies]\n",
    "            if len(error_anomalies) > 0:\n",
    "                trace_results[1] = [str_abnormal, error_anomalies]\n",
    "            if len(brier_scores) > 0:\n",
    "                trace_results[2] = [str_abnormal, brier_scores]\n",
    "\n",
    "            eval_results.append(trace_results)\n",
    "\n",
    "        return eval_results\n",
    "\n",
    "    def __preprocess_trace(self, trace):\n",
    "        sequence = TraceProcessor.get_chars(trace)\n",
    "        tokenized_trace_init = self.tokenizer(sequence, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        return tokenized_trace_init\n",
    "\n",
    "    def brier_multi(self, targets, probs):\n",
    "        return np.mean(np.sum((probs - targets) ** 2))\n",
    "\n",
    "    def mask_tokens_and_evaluate(self, tokenized_trace_init, num_tokens, mask_indices):\n",
    "        tokenized_trace = copy.deepcopy(tokenized_trace_init)\n",
    "        true_indices_20pct = [tokenized_trace['input_ids'][0][idx].item() for idx in mask_indices]\n",
    "\n",
    "        for idx in mask_indices:\n",
    "            tokenized_trace['input_ids'][0][idx] = self.mask_token_id\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**tokenized_trace).logits\n",
    "\n",
    "        predicted_probs = []\n",
    "        true_labels = []\n",
    "        for idx, true_idx in zip(mask_indices, true_indices_20pct):\n",
    "            mask_token_logits = logits[0, idx, :]\n",
    "            mask_token_probs = F.softmax(mask_token_logits, dim=-1).cpu().numpy()\n",
    "            predicted_probs.append(mask_token_probs)\n",
    "\n",
    "            true_label = np.zeros_like(mask_token_probs)\n",
    "            true_label[true_idx] = 1\n",
    "            true_labels.append(true_label)\n",
    "\n",
    "        predicted_probs_array = np.array(predicted_probs)\n",
    "        true_labels_array = np.array(true_labels)\n",
    "\n",
    "        return self.brier_multi(true_labels_array, predicted_probs_array)\n",
    "\n",
    "    def evaluate_trace_brier(self, tokenized_trace_init, tokens):\n",
    "        brier_scores = []\n",
    "        batch_encodings = [tokenized_trace_init] if len(\n",
    "            tokenized_trace_init['input_ids'][0]) <= 510 else self.split_trace_to_batch_encoding(tokenized_trace_init)\n",
    "\n",
    "        for tokenized_trace in batch_encodings:\n",
    "            num_tokens = len(tokenized_trace['input_ids'][0])\n",
    "            for _ in range(10):\n",
    "                mask_indices_20pct = np.random.choice(num_tokens, int(num_tokens * self.mask_share), replace=False)\n",
    "                brier_score = self.mask_tokens_and_evaluate(tokenized_trace, num_tokens, mask_indices_20pct)\n",
    "                if brier_score > self.brier_score_threshold:\n",
    "                    masked_tokens = [tokens[idx] for idx in mask_indices_20pct]\n",
    "                    brier_scores.append((brier_score, masked_tokens))\n",
    "        return brier_scores\n",
    "\n",
    "    def evaluate_token(self, tokenized_trace, idx):\n",
    "        true_idx = tokenized_trace['input_ids'][0][idx].item()\n",
    "        tokenized_trace['input_ids'][0][idx] = self.mask_token_id\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**tokenized_trace).logits\n",
    "        mask_token_logits = logits[0, idx, :]\n",
    "        abnormal_error = F.cross_entropy(mask_token_logits.view(1, -1).to(self.device),\n",
    "                                         torch.tensor([true_idx]).to(self.device))\n",
    "        abnormal_prob = F.softmax(mask_token_logits, dim=-1)[true_idx].item()\n",
    "        token_value = self.tokenizer.convert_ids_to_tokens(true_idx)\n",
    "        error_anomaly = (abnormal_error.item(), token_value) if abnormal_error > self.abnormal_error_threshold else None\n",
    "        prob_anomaly = (abnormal_prob, token_value) if abnormal_prob < self.abnormal_prob_threshold else None\n",
    "        return prob_anomaly, error_anomaly\n",
    "\n",
    "    def split_trace_to_batch_encoding(self, tokenized_trace):\n",
    "        max_length = 510\n",
    "        input_ids = tokenized_trace['input_ids'][0]\n",
    "        token_type_ids = tokenized_trace['token_type_ids'][0]\n",
    "        attention_mask = tokenized_trace['attention_mask'][0]\n",
    "\n",
    "        def split_component(component):\n",
    "            return [component[i:i + max_length] for i in range(0, len(component), max_length)]\n",
    "\n",
    "        input_ids_chunks = split_component(input_ids)\n",
    "        token_type_ids_chunks = split_component(token_type_ids)\n",
    "        attention_mask_chunks = split_component(attention_mask)\n",
    "\n",
    "        batch_encodings = []\n",
    "        for i in range(len(input_ids_chunks)):\n",
    "            batch_encoding = BatchEncoding({\n",
    "                'input_ids': input_ids_chunks[i].unsqueeze(0),\n",
    "                'token_type_ids': token_type_ids_chunks[i].unsqueeze(0),\n",
    "                'attention_mask': attention_mask_chunks[i].unsqueeze(0)\n",
    "            }, tensor_type='pt')\n",
    "            batch_encodings.append(batch_encoding)\n",
    "        return batch_encodings\n",
    "\n",
    "    def evaluate_trace_by_tokens(self, tokenized_trace_init, tokens):\n",
    "        error_anomalies = []\n",
    "        probs_anomalies = []\n",
    "        batch_encodings = [tokenized_trace_init] if len(\n",
    "            tokenized_trace_init['input_ids'][0]) <= 510 else self.split_trace_to_batch_encoding(tokenized_trace_init)\n",
    "\n",
    "        for tokenized_trace in batch_encodings:\n",
    "            num_tokens = len(tokenized_trace['input_ids'][0])\n",
    "            for idx in range(num_tokens):\n",
    "                tokenized_trace_copy = copy.deepcopy(tokenized_trace)\n",
    "                if tokenized_trace['input_ids'][0][idx] == self.pad_token_id:\n",
    "                    break\n",
    "                prob_anomaly, error_anomaly = self.evaluate_token(tokenized_trace_copy, idx)\n",
    "                if error_anomaly:\n",
    "                    error_anomalies.append(error_anomaly)\n",
    "                if prob_anomaly:\n",
    "                    probs_anomalies.append(prob_anomaly)\n",
    "        return probs_anomalies, error_anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65c21bcd-5b92-4c28-8a5d-9885dae12320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:24.318745Z",
     "iopub.status.busy": "2024-06-26T17:13:24.317537Z",
     "iopub.status.idle": "2024-06-26T17:13:24.443967Z",
     "shell.execute_reply": "2024-06-26T17:13:24.443136Z",
     "shell.execute_reply.started": "2024-06-26T17:13:24.318701Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "class TraceProcessor:\n",
    "    base_dir = os.path.dirname(__file__)\n",
    "    file_path_e_v = os.path.join(base_dir, 'data', 'event_codes.json')\n",
    "    event_codes = None\n",
    "\n",
    "    @classmethod\n",
    "    def initialize_event_codes(cls):\n",
    "        with open(cls.file_path_e_v, 'r', encoding='utf-8') as file:\n",
    "            cls.event_codes = json.load(file)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_event_code(event_name):\n",
    "        return TraceProcessor.event_codes.get(event_name, None)\n",
    "\n",
    "    @classmethod\n",
    "    def get_chars(cls, trace):\n",
    "        if cls.event_codes is None:\n",
    "            cls.initialize_event_codes()\n",
    "        sequence = ''\n",
    "        for event in trace:\n",
    "            event_code = cls.get_event_code(event)\n",
    "            if event_code is not None:\n",
    "                sequence += chr(event_code)\n",
    "        return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4a1e3afd-25ec-4f2e-b6e6-cb1fdb26e683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:24.748893Z",
     "iopub.status.busy": "2024-06-26T17:13:24.747729Z",
     "iopub.status.idle": "2024-06-26T17:13:24.763226Z",
     "shell.execute_reply": "2024-06-26T17:13:24.762514Z",
     "shell.execute_reply.started": "2024-06-26T17:13:24.748856Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TracesProcessor:\n",
    "    def __init__(self):\n",
    "\n",
    "        base_dir = os.path.dirname(__file__)\n",
    "        accepted_events_path = os.path.join(base_dir, 'data', 'accepted_events.json')\n",
    "\n",
    "        with open(accepted_events_path, 'r') as json_file:\n",
    "            self.accepted_events = json.load(json_file)\n",
    "\n",
    "        accepted_indexes = [i for i in range(33, 127)]\n",
    "\n",
    "        total_events = len(self.accepted_events)\n",
    "        to_add = total_events - len(accepted_indexes)\n",
    "        addition = [i for i in range(256, 256 + to_add)]\n",
    "        accepted_indexes += addition\n",
    "\n",
    "        self.event_codes = {event: accepted_indexes[index] for index, event in\n",
    "                            enumerate(self.accepted_events)}\n",
    "\n",
    "    def __get_event_code(self, event_name):\n",
    "        return self.event_codes.get(event_name, None)\n",
    "\n",
    "    def __get_sequence(self, trace):\n",
    "        sequence = ''\n",
    "        for event in trace:\n",
    "            id = self.__get_event_code(event)\n",
    "            sequence += chr(id)\n",
    "        return sequence\n",
    "\n",
    "    def __traces2seqs(self, traces):\n",
    "        return [self.__get_sequence(trace) for trace in traces]\n",
    "\n",
    "    def process_traces(self, traces, LoA):\n",
    "        sequences = self.__traces2seqs(traces)\n",
    "\n",
    "        tokenizer = TokenizerManager.get_tokenizer(LoA)\n",
    "        mapper = TokenizerManager.get_mapping(LoA)\n",
    "\n",
    "        processed_traces = []\n",
    "\n",
    "        for sequence in tqdm(sequences, desc=\"tokenizing traces\"):\n",
    "            trace = []\n",
    "\n",
    "            tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "            for token in tokens:\n",
    "                trace.append(mapper[token])\n",
    "\n",
    "            processed_traces.append(trace)\n",
    "\n",
    "        return processed_traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c77c32cb-73ad-4627-bf6a-c52f74b9c1fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:25.168291Z",
     "iopub.status.busy": "2024-06-26T17:13:25.167219Z",
     "iopub.status.idle": "2024-06-26T17:13:25.190199Z",
     "shell.execute_reply": "2024-06-26T17:13:25.189441Z",
     "shell.execute_reply.started": "2024-06-26T17:13:25.168254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def visualize_event_traces(dataframe, xes_file, LoA, stride=0.5):\n",
    "    activities = dataframe['concept:name'].unique()\n",
    "    activity_colors = {activity: plt.cm.nipy_spectral(i / len(activities) + stride) for i, activity in\n",
    "                       enumerate(activities)}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    for i, (case_id, group) in enumerate(dataframe.groupby('case:concept:name')):\n",
    "        group = group.reset_index(drop=True)\n",
    "        for j in range(len(group)):\n",
    "            ax.barh(i, 1, left=j, color=activity_colors[group.at[j, 'concept:name']], edgecolor='none')\n",
    "\n",
    "    ax.set_yticks(range(i + 1))\n",
    "    ax.set_yticklabels(dataframe['case:concept:name'].unique())\n",
    "    ax.set_xlabel('Events in Trace')\n",
    "    ax.set_title('Event Trace Visualization')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    base_dir = \"plots\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "\n",
    "    filename = FileManager.get_filename(xes_file)\n",
    "    default_filename = f\"{filename}_LoA_{LoA}.png\"\n",
    "\n",
    "    output_path = os.path.join('plots', default_filename)\n",
    "    print(f\"Saved to default {base_dir} dir.\")\n",
    "\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # img = Image.open(output_path)\n",
    "    # img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66685162-32e4-40f9-90f3-25fec612784b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:13:25.735828Z",
     "iopub.status.busy": "2024-06-26T17:13:25.734859Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the XES Anomaly Detector!\n",
      "Please select XES log file: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide a file path:  /home/jupyter/datasphere/project/logs/example_log.xes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an action: \n",
      "1. Find patterns\n",
      "2. Find anomalies\n",
      "3. Change log\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1, 2, 3, or 4:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mprocessing log...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 50/50 [00:01<00:00, 38.90it/s]\n",
      "creating dataframe: 100%|██████████| 31138/31138 [00:12<00:00, 2395.25it/s]\n",
      "analyzing anomalies by model: 100%|██████████| 25/25 [00:06<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace 1:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 2:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 3:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 4:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 5:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 6:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 7:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 8:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 9:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 10:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 11:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 12:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 13:\n",
      "  Method 1 - Status: abnormal\n",
      "  Method 2 - Status: abnormal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: abnormal\n",
      "\n",
      "Trace 14:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 15:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 16:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 17:\n",
      "  Method 1 - Status: abnormal\n",
      "  Method 2 - Status: abnormal\n",
      "  Method 3 - Status: abnormal\n",
      "\n",
      "  Final trace status: abnormal\n",
      "\n",
      "Trace 18:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 19:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 20:\n",
      "  Method 1 - Status: abnormal\n",
      "  Method 2 - Status: abnormal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: abnormal\n",
      "\n",
      "Trace 21:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 22:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 23:\n",
      "  Method 1 - Status: abnormal\n",
      "  Method 2 - Status: abnormal\n",
      "  Method 3 - Status: abnormal\n",
      "\n",
      "  Final trace status: abnormal\n",
      "\n",
      "Trace 24:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 25:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Choose an action: \n",
      "1. Find patterns\n",
      "2. Find anomalies\n",
      "3. Change log\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1, 2, 3, or 4:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "analyzing anomalies by model: 100%|██████████| 25/25 [00:00<00:00, 77.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace 1:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 2:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 3:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 4:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 5:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 6:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 7:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 8:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 9:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 10:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 11:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 12:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 13:\n",
      "  Method 1 - Status: abnormal\n",
      "  Method 2 - Status: abnormal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: abnormal\n",
      "\n",
      "Trace 14:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 15:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 16:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 17:\n",
      "  Method 1 - Status: abnormal\n",
      "  Method 2 - Status: abnormal\n",
      "  Method 3 - Status: abnormal\n",
      "\n",
      "  Final trace status: abnormal\n",
      "\n",
      "Trace 18:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 19:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 20:\n",
      "  Method 1 - Status: abnormal\n",
      "  Method 2 - Status: abnormal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: abnormal\n",
      "\n",
      "Trace 21:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 22:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 23:\n",
      "  Method 1 - Status: abnormal\n",
      "  Method 2 - Status: abnormal\n",
      "  Method 3 - Status: abnormal\n",
      "\n",
      "  Final trace status: abnormal\n",
      "\n",
      "Trace 24:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Trace 25:\n",
      "  Method 1 - Status: normal\n",
      "  Method 2 - Status: normal\n",
      "  Method 3 - Status: normal\n",
      "\n",
      "  Final trace status: normal\n",
      "\n",
      "Choose an action: \n",
      "1. Find patterns\n",
      "2. Find anomalies\n",
      "3. Change log\n",
      "4. Exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pm4py\n",
    "\n",
    "from transformers import SqueezeBertForMaskedLM, PreTrainedTokenizerFast\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class UserInteractionHandler:\n",
    "    def __init__(self):\n",
    "        self.model, self.tokenizer = self.__load_model_and_tokenizer()\n",
    "        self.log_processor = None\n",
    "\n",
    "    def __load_model_and_tokenizer(self):\n",
    "        base_dir = os.path.dirname(__file__)\n",
    "        model_dir = os.path.join(base_dir, 'model', 'squeezebert')\n",
    "        tokenizer_dir = os.path.join(base_dir, 'model', 'tokenizer')\n",
    "        model = SqueezeBertForMaskedLM.from_pretrained(model_dir)\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_dir)\n",
    "        return model, tokenizer\n",
    "\n",
    "    def __find_patterns(self, LoA):\n",
    "        if not self.log_processor:\n",
    "            self.log_processor = LogProcessor(self.xes_file)\n",
    "\n",
    "        tokenized_log = self.log_processor.create_tokenized_event_log(LoA=LoA)\n",
    "        dataframe = log_converter.apply(tokenized_log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "        visualize_event_traces(dataframe, self.xes_file, LoA)\n",
    "        self.save_tokenized_event_log(dataframe, LoA)\n",
    "\n",
    "    def __find_anomalies(self):\n",
    "        if not self.log_processor:\n",
    "            self.log_processor = LogProcessor(self.xes_file)\n",
    "\n",
    "        traces = self.log_processor.get_traces()\n",
    "        \n",
    "        base_dir = os.path.dirname(__file__)\n",
    "        db_dir = os.path.join(base_dir, 'db')\n",
    "        \n",
    "        if not os.path.exists(db_dir):\n",
    "            os.makedirs(db_dir)\n",
    "            \n",
    "        dp_path = os.path.join(db_dir, \"trace_evaluator.db\")\n",
    "        \n",
    "        te = TraceEvaluatorDB(self.model, self.tokenizer, db_path=dp_path)\n",
    "        eval_results = te.evaluate_traces(traces)\n",
    "        self.display_anomalies(eval_results)\n",
    "\n",
    "    def save_tokenized_event_log(self, dataframe, LoA):\n",
    "        print(\"Do you want to save tokenized event log? \")\n",
    "        print(\"1. Yes\")\n",
    "        print(\"2. No\")\n",
    "        action = int(input(\"Enter 1, 2: \"))\n",
    "\n",
    "        if action == 1:\n",
    "            filename = FileManager.get_filename(self.xes_file)\n",
    "            default_filename = f\"{filename}_LoA_{LoA}.xes\"\n",
    "            output_path = FileManager.get_save_path(default_filename, \".xes\")\n",
    "            pm4py.write_xes(dataframe, output_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def display_anomalies(results):\n",
    "        for trace_idx, trace_result in enumerate(results, 1):\n",
    "            print(f\"Trace {trace_idx}:\")\n",
    "            rate = 0\n",
    "            for method_idx, (status, details) in enumerate(trace_result, 1):\n",
    "                if status == 'abnormal':\n",
    "                    rate += 1\n",
    "                print(f\"  Method {method_idx} - Status: {status}\")\n",
    "            \n",
    "            if rate >= 2:\n",
    "                print(\"\\n  Final trace status: abnormal\")\n",
    "            else:\n",
    "                print(\"\\n  Final trace status: normal\")\n",
    "            print()\n",
    "\n",
    "    @staticmethod\n",
    "    def request_level_of_abstraction():\n",
    "        while True:\n",
    "            try:\n",
    "                LoA = int(input(\"Please enter the level of abstraction (1-13): \"))\n",
    "                if LoA < 1 or LoA > 13:\n",
    "                    raise ValueError\n",
    "                return LoA\n",
    "            except ValueError:\n",
    "                print(\"Invalid level of abstraction. Please enter a number between 1 and 13.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_file_path(xes_file):\n",
    "        if not os.path.exists(xes_file):\n",
    "            print(f\"File '{xes_file}' does not exist. Please try again.\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __get_log(self):\n",
    "        print(\"Please select XES log file: \")\n",
    "        xes_file = FileManager.get_in_path(\"xes-anomaly-detector\", \"xes\")\n",
    "        if self.validate_file_path(xes_file):\n",
    "            self.xes_file = xes_file\n",
    "        else:\n",
    "            print(\"File is not valid\")\n",
    "\n",
    "    def process_action(self, action):\n",
    "        if action == 1:\n",
    "            LoA = self.request_level_of_abstraction()\n",
    "            self.__find_patterns(LoA)\n",
    "        elif action == 2:\n",
    "            self.__find_anomalies()\n",
    "        elif action == 3:\n",
    "            self.__get_log()\n",
    "        elif action == 4:\n",
    "            print(\"Exiting the program.\")\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 1, 2, 3, or 4.\")\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Welcome to the XES Anomaly Detector!\")\n",
    "        self.__get_log()\n",
    "\n",
    "        if not self.xes_file:\n",
    "            return\n",
    "\n",
    "        while True:\n",
    "            print(\"Choose an action: \")\n",
    "            print(\"1. Find patterns\")\n",
    "            print(\"2. Find anomalies\")\n",
    "            print(\"3. Change log\")\n",
    "            print(\"4. Exit\")\n",
    "            action = int(input(\"Enter 1, 2, 3, or 4: \"))\n",
    "\n",
    "            self.process_action(action)\n",
    "            \n",
    "            if action == 4:\n",
    "                return\n",
    "\n",
    "\n",
    "def main():\n",
    "    handler = UserInteractionHandler()\n",
    "    handler.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f983c7b-36d2-436b-a831-85926e8ab4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
